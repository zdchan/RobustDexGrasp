seed: 1

environment:

  hardware:
    kinematic_type: "simfk"
    arm_type: "ur5"                
    hand_type: "allegro"
    real_world_mode: False          
    # Enables hand-only mode without the arm
    flying_hand_mode: False
    # Option to save simulation states
    save_state_mode: False
    # Resource model identifier
    rsc_model: "ur5_allegro"
    # Simulation model path reference - rsc/${rsc_model}/${sim_model}.urdf
    sim_model: "ur5_allegro"

    # Collision group ID for visualization in RaiSim
    vis_group_id: "0"
    # Collision mask IDs - 2:table, 63:all objects
    vis_mask_id:  "0,1,2,63"
    # PD controller parameters file for the arm
    arm_pd_file: "UR5Identification_id5hz.txt" # path in ./raisimGymTorch/env/hardware/arm/UR5Identification_id5hz.txt
    # PD controller parameters file for the hand
    hand_pd_file: "Allegrotemp.txt" # path in ./raisimGymTorch/env/hardware/hand/Allegrotemp.txt
    # Initial joint positions for the 16 finger joints [rad]
    init_finger_pose: [0.2, 0.6, 0.2, 0.5, 0.2, 0.6, 0.2, 0.5, 0.2, 0.6, 0.2, 0.5, 1.3, 0.0, -0.1, 0.2]
    # Friction coefficient of the table surface
    table_friction: 0.2
    # Grasping frame position in hand frame
    hand_center: [0.0, 0.0, 0.0]

  # Enable biased point cloud observation
  biased: False
  # Number of steps for grasp sequence
  grasp_steps: 70
  # Use top-down grasping
  top: False
  # Use non-uniform sampling for object positions during training
  non_uniform_sampling: True  
  # Run evaluation during training
  eval_during_training: False
  # Camera position 
  camera_position: [0.035, -0.58, 1.531]
  # Number of samples of palm direction
  sample_num: 10
  # Coefficients for scoring pre-grasp pose quality
  length_score_coeff: 5
  angle_score_coeff: 1

  # Save checkpoint frequency
  eval_every_n: 500
  # Policy update frequency (every n iterations)
  update_every_n: 20
  # Number of parallel simulation threads
  num_threads: 16
  # Simulation time step [s]
  simulation_dt: 0.01
  # Control time step [s] (policy decision frequency)
  control_dt: 0.2
  # Maximum episode duration [s]
  max_time: 4.0
  # Enable visualization
  render: True
  visualize: False
  # Dataset to load for training
  load_set: "new_training_set"
  # Standard deviation for finger action sampling
  finger_action_std: 0.015
  # Standard deviation for arm action sampling
  rot_action_std: 0.005
  
  # Reward components and their coefficients
  reward:
    # Positive rewards for successful manipulation
    affordance_reward:
      coeff: 0.5                # Reward for joint distance to object points
    affordance_contact_reward:
      coeff: 1.5                # Reward for making contact with objects
    affordance_impulse_reward:
      coeff: 1.0                # Reward based on x-y direction impulse with objects
    
    # Negative rewards for table interaction
    table_reward:
      coeff: -0.03              # Penalty for proximity to table
    table_contact_reward:
      coeff: -1.0               # Penalty for making contact with table
    table_impulse_reward:
      coeff: -0.5               # Penalty based on impulse with table

    # Penalties for arm behavior
    arm_height_reward:
      coeff: -0.05              # Penalty for arm being too low
    arm_contact_reward:
      coeff: -0.1               # Penalty for arm contact with table
    arm_impulse_reward:
      coeff: -0.1               # Penalty based on arm impulses with table
    arm_collision_reward:
      coeff: -1.0               # Penalty for arm self-collisions
      
    # Object interaction penalties
    push_reward:
      # coeff: -15.0              # Penalty for pushing objects to the table
      coeff: -0.0              # Penalty for pushing objects to the table

    # Velocity and motion penalties
    wrist_vel_reward_:
      coeff: -1.0               # Penalty for high wrist linear velocity
    wrist_qvel_reward_:
      coeff: -0.1               # Penalty for high wrist angular velocity
    obj_vel_reward_:
      coeff: -15.0              # Severe penalty for high object linear velocity
    obj_qvel_reward_:
      coeff: -0.2               # Penalty for high object angular velocity
    obj_displacement_reward:
      coeff: -5.0               # Penalty for object displacement
    arm_joint_vel_reward_:
      coeff: -1.0               # Penalty for high arm joint velocities

# Neural network architecture
architecture:
  policy_net: [128, 128]
  value_net: [128, 128]
